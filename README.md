# Predicting Team Performance in FIFA World Cups

## Research Question
Can national teamsâ€™ progression in FIFA World Cups be predicted using pre-tournament characteristics and group-stage performance data?

## Set Up

### Create environment
conda env create -f environment.yml
conda activate worldcup-project
### Usage
python main.py
Running the main script prints model evaluation results to the console, including:

- Training and test set sizes
- Class distribution in the test set
- Accuracy and Macro F1-score for each model
- Detailed classification reports
- Confusion matrices (raw counts and row-normalized percentages)
- Identification of the best-performing model on the test set

# Project Structure
Road-to-glory/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ clean/               # Cleaned and processed dataset
â”‚       â””â”€â”€ final_dataset1.csv
â”‚   â””â”€â”€ raw/              # Original data sources       
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ cleaning_data.ipynb
â”‚   â””â”€â”€ EDA_Results_Road_to_Glory.ipynb
â”‚
â”œâ”€â”€ Report
â”‚    â””â”€â”€ project_report.pdf
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ metrics.csv
â”‚   â””â”€â”€ plots/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # Feature sets and constants
â”‚   â”œâ”€â”€ data_loader.py      # Data loading and splitting logic
â”‚   â”œâ”€â”€ evaluation.py       # Evaluation utilities
â”‚   â””â”€â”€ models.py           # Model training functions
â”‚
â”œâ”€â”€ AI_USAGE.md
â”œâ”€â”€ environment.yml
â”œâ”€â”€ main.py                 # End-to-end training and evaluation script
â””â”€â”€ README.md

## Results

Four models were evaluated on the test set: Logistic Regression and XGBoost, each with a baseline
and an enriched feature set.

The best overall performance is achieved by the XGBoost baseline model, which uses group-stage
performance features only. It reaches an accuracy of 0.641 and a Macro F1-score of 0.449, clearly
outperforming all other configurations.

Logistic Regression provides a strong interpretable baseline but struggles with intermediate and
advanced tournament stages. In contrast, XGBoost substantially improves performance on Quarter-final, Semi-final, and Final outcomes, which explains the large increase in Macro F1-score.

Adding pre-tournament features (strength, host status, average age) does not improve performance
and in some cases degrades it, likely due to increased noise and limited sample size. This result
is consistent with correlation analysis, which shows that group-stage performance variables dominate the predictive signal.

Performance on the Winner class remains limited due to the very small number of observations in the
test set, which is a structural limitation of the dataset.

### Report
ðŸ“„ Final report: `report/project_report.pdf`

## Requirements
- Python 3.11
- scikit-learn, pandas, numpy, matplotlib, seaborn, xgboost, pathlib


